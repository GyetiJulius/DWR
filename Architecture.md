Architectures for Dynamic Weight Retrieval
    • Mixture of Experts (MoE) architectures sparsely activate large pools of parameters by routing each token to a few “expert” sub-networks. Early MoE models (Shazeer et al., 2017) already required auxiliary “load-balancing” losses because learned routers tend to collapse onto a few experts[1][2]. Modern large MoE transformers (Google’s Switch Transformer, GShard, GLaM, etc.) use gating networks to select top-k experts (e.g. top-1 or top-2) per token and include balancing terms to encourage uniform expert utilization[3][2]. For example, Switch Transformer uses a learned router that picks one expert per token and adds a balancing loss term to avoid expert collapse[3]. These designs effectively decouple the model structure (the router and expert indices) from the expert weight storage, enabling dynamic expert loading.
    • Retrieval-augmented Transformers (e.g. RETRO) decouple “memory” from model weights by retrieving relevant data chunks at runtime. DeepMind’s RETRO model shows that an autoregressive transformer can achieve GPT-3–level performance by conditioning on retrieved text from an external 2-trillion-token corpus[4]. In RETRO, the model structure (encoder/decoder) is separate from the retrieval database; at inference time it dynamically fetches and attends retrieved passages rather than storing all knowledge in parameters. This “augmented memory” is a form of dynamic weight retrieval: only needed information is brought in for each input.
    • On-Demand Parameter Server (ODC) and related schemes revisit the classic parameter-server paradigm for large models. Wan et al. (2026) propose On-Demand Communication (ODC) for LLM post-training, replacing layer-by-layer all-gather in FSDP with point-to-point transfers[5]. ODC treats shards of the model as if on a parameter server, fetching parameters only when needed (once per microbatch instead of per layer), which alleviates stragglers under imbalanced sequence lengths[5]. This effectively separates the storage of parameter shards from the compute graph, allowing asynchronous or on-demand retrieval of model weights during training.
Runtime Systems and Memory Management
    • Weight Streaming / Offloading Frameworks: Modern inference engines stream weights from slower storage (CPU/NVMe/S3) to the GPU as needed. NVIDIA’s TensorRT supports weight streaming: a Torch-TensorRT example shows how a compiled Llama-2 engine can keep weights in host memory and stream them to the GPU during execution, enabling models larger than GPU RAM[6]. Similarly, AWS’s SageMaker Fast Model Loader streams model weights directly from S3 to GPU via DMA, eliminating intermediary disk writes[7]. These systems load weights in small chunks (e.g. 8 MB shards) and overlap loading with compute. As a result, GPUs can handle model sizes (or batch sizes) beyond their physical memory by asynchronously fetching parameters, at the cost of some throughput. IBM’s Pie framework further extends this idea on NVIDIA’s GraceHopper (GH200) architecture: by using performance-transparent swapping, Pie prefetches data over NVLink so GPU compute never stalls, effectively “extending” GPU memory with host RAM[8].
    • CPU↔GPU Offload and Memory Paging: Training frameworks like DeepSpeed ZeRO-Offload and PyTorch FSDP partition parameters, optimizer states, and gradients across CPU/GPU to fit giant models[5]. Accelerate’s load_checkpoint_and_dispatch utility automates this: it shards weights across available GPUs/CPU and even memory-maps excess parameters to disk[9]. Inference engines like FlexGen similarly aggregate GPU, CPU, and disk: FlexGen formulates an LP to optimally offload weights and KV-cache entries, and even compresses them to 4-bit, enabling e.g. OPT-175B on a single 16 GB GPU[10]. The open-source NEO system takes the opposite approach: it keeps weights on GPU but offloads attention computation and key-value cache to the CPU, using asynchronous GPU/CPU pipelining to hide latency[11][12]. These systems demonstrate a spectrum of weight management: fully externalized (paging/streaming) vs. hybrid (scatter/load-balance between memories).
    • CUDA Memory Techniques: CUDA’s Unified Memory and explicit prefetching are key enablers. NVIDIA notes that Unified Memory lets the GPU access any page of system memory and migrate needed pages on-demand[13]. In practice, applications can use cudaMallocManaged with cudaMemPrefetchAsync to overlap data transfers with compute[14]. For example, prefetching data to the GPU can double achievable host-to-device bandwidth compared to purely on-demand migration[14]. In other words, careful use of CUDA managed memory (prefetching, multiple streams, page-access patterns) allows weight and activation pages to be pulled from CPU memory without blocking kernels. Other low-level techniques like GPUDirect (RDMA) and memory-mapped checkpoints also support dynamic retrieval: huggingface Accelerate, for instance, by default loads safetensors via mmap to avoid full loads into RAM.
Training Strategies for Sparse Models
    • Load Balancing: To prevent experts from receiving no data, MoE training typically adds an auxiliary loss. The Switch Transformer introduced a load-balancing loss that encourages each expert to receive roughly equal aggregate probability[3]. PyTorch documentation explains that without such regularization, some experts go unused[3][2]. The recent SimBal approach goes further: instead of forcing a uniform distribution, it adds a regularizer that preserves similarity between tokens routed to the same expert[15]. SimBal softly enforces orthogonality in the router weights so that semantically similar tokens map to similar expert distributions, accelerating convergence and reducing redundant learning[15]. In practice, balancing often combines an importance loss (making router logits non-peaky) with a load loss (equalizing per-expert token counts) as seen in many MoE works[3][2].
    • Expert Specialization: Beyond balancing, researchers encourage experts to specialize. One approach (Krishnamurthy et al., 2023) modifies the gating network to include expert outputs (“attentive gating”), which improved task decomposition and reduced entropy of routing[16]. Data-driven regularizations can also help: for example, Krishnamurthy et al. introduce a regularizer that pushes the router to assign similar inputs to the same expert, yielding cleaner specialization. Generally, these strategies aim to let each expert focus on a subset of the data (e.g. certain topics or syntactic patterns) rather than all experts learning redundant functions.
    • Efficient Routing and Computation: Many MoE libraries optimize the sparse routing computation. A common scheme is top-k routing: the router computes scores for all M experts and selects the highest-k (often k=1 or 2). Techniques like NVIDIA’s MegaBlocks implement “dropless” routing: they avoid dropping excess tokens by using block-sparse matrix multiplications that can handle uneven token loads[17]. MegaBlocks allows each expert to process a variable number of tokens without padding, and uses a custom GPU kernel to do all selected experts’ matmuls in parallel[17]. This eliminates the trade-off between model quality and efficiency (previous schemes would drop tokens if too many routed to one expert). In summary, routing strategies combine algorithmic choices (top-k, gating architecture) with system optimizations (sparse GPU kernels) to handle sparse activation efficiently.
Frameworks and Implementations
    • PyTorch & Hugging Face: PyTorch’s ecosystem directly supports these methods. The Accelerate library’s init_empty_weights() and load_checkpoint_and_dispatch() let you build a model skeleton on CPU and then stream weights onto GPUs (or disk) as needed[9]. Hugging Face Transformers provides a WeightConverter utility to reshape and distribute weights for model parallelism: for MoE models, it can merge multiple expert tensors into one stacked tensor or split them back into individual experts[18][19]. This makes it easy to load quantized or parallel-sharded MoE checkpoints by defining composable conversion ops. The HF Optimum and Accelerate toolkits also integrate offload/backing stores and serve large models with limited GPU RAM.
    • DeepSpeed and FSDP: Microsoft’s DeepSpeed offers ZeRO-inference and ZeRO-training optimizations that shard parameters across devices and allow CPU/NVMe offload (ZeRO-Offload) to train models with tens of billions of parameters on small GPU clusters. Although not always termed “dynamic retrieval,” ZeRO effectively fetches shards into GPU memory when used in forward/back passes. The ODC work mentioned above even builds directly on FSDP to allow point-to-point weight fetching, reducing synchronization overhead in large-batch LLM training[5].
    • Inference Engines: Several open-source engines specialize in dynamic memory management. FlexGen (Sheng et al., 2023) treats LLM inference as an optimization problem, spilling weights/caches to CPU or disk and using 4-bit compression to maximize throughput[10]. vLLM (Shen et al., 2023) focuses on efficient KV-cache management and unified memory usage to speed up batched LLM serving. NEO (MLSys 2025) demonstrates CPU offloading of KV cache for multi-request inference with load-aware scheduling[11][12]. On the hardware side, NVIDIA’s Triton Inference Server and TensorRT provide C++ backends for model serving; TensorRT’s weight streaming and dynamic memory APIs let production systems load huge models across CPU/GPU.
    • Other Libraries: The Mixtral and DeepSeek projects (Databricks) and FairScale/Colossal-AI also include MoE layers and support sparse routing. NVIDIA’s FasterTransformer library offers high-performance kernels (including MoE) for transformer inference. In addition, many teams rely on quantization libraries (e.g. 8-bit or 4-bit compress) to reduce memory footprint of weights and caches as part of these systems.
Hardware-Aware and Deployment Optimizations
    • High-Bandwidth CPU–GPU Links: Modern accelerator designs blur the CPU/GPU memory boundary. NVIDIA’s Grace Hopper Superchip (GH200) pairs a CPU and GPU with NVLink-C2C, giving CPUs direct high-speed access to HBM2e on the GPU. Pie’s design explicitly targets GH200: by leveraging its high-bandwidth interconnect, Pie overlaps GPU compute with CPU–GPU data movement, achieving “transparent” memory expansion[20]. In general, GPUs with NVLink or NVSwitch allow much faster Unified Memory than PCIe: Sakharnykh et al. show that on a PCIe system streaming peaks at ~5 GB/s via on-demand migration, whereas NVLink can reach ~10 GB/s with prefetching[14]. This hardware support makes dynamic paging far more practical.
    • Memory-Disaggregated Architectures: Some specialized chips are built for weight streaming. Cerebras’s wafer-scale machines use MemoryX (pooling memory across chips) and SwarmX (scalable interconnect) to “disaggregate” memory from compute. Feldman (2022) explains that Cerebras clusters can stream model weights from remote memories across the wafer network, yielding near-linear scaling as model size grows[21]. In essence, Cerebras treats weights as streaming data fed to a massive compute fabric. Similarly, Graphcore IPUs and Habana Gaudi processors include very large on-chip memory and fast interconnects to support model-parallel and swapping workloads.
    • Quantization and On-the-Fly Compute: Hardware features like Tensor Cores (for INT8/4) also enable storing weights in compressed form and reconstructing them on-the-fly. Systems often load 4‑bit quantized weights into GPU or CPU memory and expand them during matmul. CUDA’s __half/__int8 instructions and even specialized chips can directly compute from low-bit weights, effectively trading compute for memory. Some proposals even use hypernetworks or weight generators to compute parameters as needed rather than storing them explicitly. Finally, deployment techniques like NVIDIA’s CUDA Graphs (capturing entire sequence of operations) and container caching (as in SageMaker) reduce redundant weight transfers, further optimizing on-demand loading.
Sources: Concepts and examples are drawn from recent papers and system reports, including Hugging Face Transformers documentation[18][19], DeepMind’s RETRO study[4], MoE training research (e.g. SimBal[15], attentive MoE[16]), NVIDIA and AWS blogs on memory management[6][7][13], and recent infrastructure papers (Pie[8], FlexGen[10], NEO[11], ODC[5]). Each source is cited above in context.

[1] [3] [15] Load Balancing Mixture of Experts with Similarity Preserving Routers
https://arxiv.org/html/2506.14038v1
[2] [17] Training MoEs at Scale with PyTorch – PyTorch
https://pytorch.org/blog/training-moes/
[4] [2112.04426] Improving language models by retrieving from trillions of tokens
https://arxiv.org/abs/2112.04426
[5] Revisiting Parameter Server in LLM Post-Training
https://arxiv.org/html/2601.19362v1
[6] Weight Streaming — Torch-TensorRT v2.12.0.dev0+66e7b2a documentation
https://docs.pytorch.org/TensorRT/tutorials/_rendered_examples/dynamo/weight_streaming_example.html
[7] Introducing Fast Model Loader in SageMaker Inference: Accelerate autoscaling for your Large Language Models (LLMs) – part 1 | Artificial Intelligence
https://aws.amazon.com/blogs/machine-learning/introducing-fast-model-loader-in-sagemaker-inference-accelerate-autoscaling-for-your-large-language-models-llms-part-1/
[8] [20] Pie: Pooling CPU Memory for LLM Inference
https://arxiv.org/html/2411.09317v1
[9] Loading big models into memory
https://huggingface.co/docs/accelerate/main/concept_guides/big_model_inference
[10] [2303.06865] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU
https://arxiv.org/abs/2303.06865
[11] [12] GitHub - MachineLearningSystem/25MLSYS-NEO: NEO is a LLM inference engine built to save the GPU memory crisis by CPU offloading
https://github.com/MachineLearningSystem/25MLSYS-NEO
[13] [14] Maximizing Unified Memory Performance in CUDA | NVIDIA Technical Blog
https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/
[16] [2302.14703] 1 Introduction
https://ar5iv.labs.arxiv.org/html/2302.14703
[18] [19] Dynamic weight loading
https://huggingface.co/docs/transformers/en/weightconverter
[21] Linear Scaling Made Possible with Weight Streaming - Cerebras
https://www.cerebras.ai/blog/linear-scaling-made-possible-with-weight-streaming